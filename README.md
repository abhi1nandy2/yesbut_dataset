# Code for the paper - "***YesBut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models"

https://drive.google.com/file/d/1s5K0FlUOKUKknhKh9runmjDKouIAVxwM/view?usp=sharing - contains the 283 images manually downloaded (and then manually filtered) from the posts in ‘X’ (erstwhile known as Twitter) handle @\_yesbut\_.

- Links for running the SOTA VL Models
  - `LLaVA` - https://github.com/haotian-liu/LLaVA
  - `MiniGPT4` - https://github.com/Vision-CAIR/MiniGPT-4
  - `Kosmos-2` - https://github.com/microsoft/unilm/tree/master/kosmos-2
  - `GPT4` - https://platform.openai.com/docs/guides/vision (We use `gpt-4-vision-preview` API)
  - `Gemini` - https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini#gemini-1.0-pro-vision
- `generate_using_dalle3.ipynb` - contains the code for generating images using DALL-E 3
